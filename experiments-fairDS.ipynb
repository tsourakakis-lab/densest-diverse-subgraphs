{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook partially relies on code+data from [1] that are not publicly available. \n",
    "\n",
    "[1] Aris Anagnostopoulos, Luca Becchetti, Adriano Fazzone, Cristina Menghini, and Chris Schwiegelshohn. 2020. Spectral Relaxations and Fair Densest Subgraphs. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (CIKM '20). Association for Computing Machinery, New York, NY, USA, 35â€“44. DOI:https://doi.org/10.1145/3340531.3412036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import binarize\n",
    "from algorithms import *\n",
    "import dsd\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketplaces = [f for f in listdir('../../CIKM20__Spectral_Relaxations_and_Fair_Densest_Subgraphs__sw/data/Amazon_metadata_2018') if ( isfile(join('../../CIKM20__Spectral_Relaxations_and_Fair_Densest_Subgraphs__sw/data/Amazon_metadata_2018', f)) and f.split('.')[1]=='tsv')]\n",
    "datasets = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_amazon( uni_name ):\n",
    "    prefix = '../../CIKM20__Spectral_Relaxations_and_Fair_Densest_Subgraphs__sw/data/Amazon_metadata_2018/'\n",
    "    product_map = dict()\n",
    "    prod = 0\n",
    "    category_map = dict()\n",
    "    cat = 0\n",
    "    edges = []\n",
    "    products_labels = dict()\n",
    "    with open(prefix + uni_name + '.tsv','r') as f:\n",
    "        first_line = f.readline()\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            id1, id2, cat1, cat2 = line.split('\\t')\n",
    "            if id1 not in product_map:\n",
    "                product_map[id1] = prod\n",
    "                prod += 1\n",
    "            if id2 not in product_map:\n",
    "                product_map[id2] = prod\n",
    "                prod += 1\n",
    "            if cat1 not in category_map:\n",
    "                category_map[cat1] = cat\n",
    "                cat += 1\n",
    "            if cat2 not in category_map:\n",
    "                category_map[cat2] = cat\n",
    "                cat += 1\n",
    "            # Add edge\n",
    "            edges.append([product_map[id1],product_map[id2]])\n",
    "            # Create labels\n",
    "            products_labels[product_map[id1]] = category_map[cat1]\n",
    "            products_labels[product_map[id2]] = category_map[cat2]\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from( edges )\n",
    "    labels = [products_labels[i] for i in range(G.number_of_nodes())]\n",
    "    return nx.to_scipy_sparse_matrix(G),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for uni in marketplaces:\n",
    "    if True:\n",
    "        uni_name = uni.split('.')[0]\n",
    "        datasets[uni_name] = dict()\n",
    "        graph,labels = read_amazon( uni_name )\n",
    "        datasets[uni_name]['graph'] = graph\n",
    "        datasets[uni_name]['labels'] = labels\n",
    "        #datasets[uni_name]['G'] = nx.from_numpy_matrix(mat_contents['A'])\n",
    "    #except:\n",
    "    #    print(\"Error\")\n",
    "    #    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_size = defaultdict( int )\n",
    "ds_dist = defaultdict( list )\n",
    "algo2_size = dict()\n",
    "algo2_dist = dict()\n",
    "ps_size = defaultdict( int )\n",
    "ps_dist = defaultdict( list )\n",
    "#fps_size = defaultdict( int )\n",
    "fps_dist = defaultdict( list )\n",
    "ds_connected = defaultdict( bool )\n",
    "algo2_connected = dict()\n",
    "ps_connected = defaultdict( bool )\n",
    "fps_connected = defaultdict( bool )\n",
    "\n",
    "ds_diameter = defaultdict( int )\n",
    "algo2_diameter = dict()\n",
    "ps_diameter = defaultdict( int )\n",
    "fps_diameter = defaultdict( int )\n",
    "\n",
    "ds_nodes = defaultdict( int )\n",
    "algo2_nodes = dict()\n",
    "ps_nodes = defaultdict( int )\n",
    "fps_nodes = defaultdict( int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelsDist( S, l ):\n",
    "    maxL = max(l) + 1\n",
    "    dist = [0 for _ in range(maxL)]\n",
    "    for n in S:\n",
    "        dist[l[n]] += 1\n",
    "    return dist\n",
    "\n",
    "def diameter_estimation( G, S ):\n",
    "    # G shoud be the subgraph -- not the graph!\n",
    "    samples = max(int(0.1*len(S)),min(100,len(S)))\n",
    "    src_list = random.sample( S, samples)\n",
    "    diameter = 0\n",
    "    for src in src_list:\n",
    "        res = nx.single_source_shortest_path_length(G, src)\n",
    "        max_val = max(res.values())\n",
    "        diameter = max(diameter,max_val)\n",
    "    return diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get largest connected component\n",
    "dtsts_cnt = 0\n",
    "for uni_name in datasets:\n",
    "    if uni_name not in fps_size:\n",
    "        continue\n",
    "    try:\n",
    "        print(\"---\", uni_name, \"----\")\n",
    "        G = nx.from_numpy_matrix(datasets[uni_name]['graph'])\n",
    "        Gcc = max(nx.connected_components(G), key=len)\n",
    "        Gcc = sorted(list(Gcc))\n",
    "        datasets[uni_name]['graph'] = datasets[uni_name]['graph'][np.ix_(list(Gcc),list(Gcc))]\n",
    "        datasets[uni_name]['labels'] = [datasets[uni_name]['labels'][i] for i in Gcc]\n",
    "        G = G.subgraph(Gcc)\n",
    "        G = nx.convert_node_labels_to_integers(G)\n",
    "        labels = list(datasets[uni_name]['labels'])\n",
    "        if max(labels)+1 == 2:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        # DSD\n",
    "        edges = [e for e in G.edges()]\n",
    "        flowless_result = dsd.flowless(edges, 4)\n",
    "        DS, DS_density = flowless_result[0], flowless_result[1]\n",
    "        ds_size[uni_name] = DS_density\n",
    "        ds_dist[uni_name] = getLabelsDist( DS, labels )\n",
    "        print(\"Densest Subgraph: {:.2f}\".format(DS_density))\n",
    "\n",
    "        ds_nodes[uni_name] = len(DS)\n",
    "        # Connectivity & Diameter\n",
    "        G_DS = G.subgraph(DS)\n",
    "        ds_connected[uni_name] = nx.is_connected( G_DS )\n",
    "        if ds_connected[uni_name]:\n",
    "            ds_diameter[uni_name] = diameter_estimation( G_DS, DS )\n",
    "\n",
    "    #     # dalkDS and dalkFairSubgraph\n",
    "        algo2_size[uni_name] = dict()\n",
    "        algo2_dist[uni_name] = dict()\n",
    "        algo2_connected[uni_name] = dict()\n",
    "        algo2_diameter[uni_name] = dict()\n",
    "        algo2_nodes[uni_name] = dict()\n",
    "        for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "            print(\"Results for a = {}\".format(a))\n",
    "            try:\n",
    "                FP, dalk_density = Algorithm2( G, labels, a )\n",
    "                fairDS, colors = FP\n",
    "                t = len(colors)\n",
    "                fairDensity = G.subgraph(fairDS).size() / len(fairDS)\n",
    "                print(\"Densest at least-k: {:.2f}\".format(dalk_density))\n",
    "                print(\"Fair at most alpha density: {:.2f}\".format(fairDensity))\n",
    "                print(\"Maximum group contribution: {:.2f}\".format(max([len(v) for k,v in colors.items()])/len(fairDS)))\n",
    "                print(\"Minimum group contribution: {:.2f}\".format(min([len(v) for k,v in colors.items()])/len(fairDS)))\n",
    "                size = fairDensity\n",
    "                dist = getLabelsDist( fairDS,labels )\n",
    "\n",
    "    #             # Connectivity & Diameter\n",
    "                algo2_nodes[uni_name][a] = len(fairDS)\n",
    "                G_DS = G.subgraph(list(fairDS))\n",
    "                algo2_connected[uni_name][a] = nx.is_connected( G_DS )\n",
    "                if algo2_connected[uni_name][a]:\n",
    "                    algo2_diameter[uni_name][a] = diameter_estimation( G_DS, fairDS )\n",
    "            except:\n",
    "                print(\"Bug or not feasible solution under current setting.\")\n",
    "                size = 0\n",
    "                dist = [0 for _ in range(max(labels)+1)]\n",
    "            algo2_size[uni_name][a] = size\n",
    "            algo2_dist[uni_name][a] = dist\n",
    "        # Paired Sweep from Anagnostopoulos\n",
    "        ps_ds, ps_size[uni_name] = pairedSweep(datasets[uni_name]['graph'], G, labels, fair=False )\n",
    "        # Connectivity & Diameter\n",
    "        ps_nodes[uni_name] = len(ps_ds)\n",
    "        G_DS = G.subgraph(list(ps_ds))\n",
    "        ps_connected[uni_name] = nx.is_connected( G_DS )\n",
    "        if ps_connected[uni_name]:\n",
    "            ps_diameter[uni_name] = diameter_estimation( G_DS, ps_ds )\n",
    "\n",
    "        fps_ds, fps_size[uni_name] = pairedSweep(datasets[uni_name]['graph'], G, labels, fair=True )\n",
    "        # Connectivity & Diameter\n",
    "        fps_nodes[uni_name] = len(fps_ds)\n",
    "        G_DS = G.subgraph(list(fps_ds))\n",
    "        fps_connected[uni_name] = nx.is_connected( G_DS )\n",
    "        if fps_connected[uni_name]:\n",
    "            fps_diameter[uni_name] = diameter_estimation( G_DS, fps_ds )\n",
    "\n",
    "        print(\"PS and FPS \", ps_size[uni_name],fps_size[uni_name])\n",
    "        print(\"***********\")\n",
    "        dtsts_cnt += 1\n",
    "        print(\"Datasets count \",dtsts_cnt)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare algorithms and plot\n",
    "cnt = 0\n",
    "for uni_name in datasets:\n",
    "    dataset = uni_name\n",
    "    # DS\n",
    "    ds = ds_size[dataset]\n",
    "    max_a = max(ds_dist[dataset])/sum(ds_dist[dataset])\n",
    "    print(max_a)\n",
    "    plt.plot(max_a,ds,label='DS', color='black',marker='x',markersize=12)\n",
    "\n",
    "    x,y = [],[]\n",
    "    for a,val in algo2_size[dataset].items():\n",
    "        if val > 0:\n",
    "            next_x = max(algo2_dist[dataset][a])/sum(algo2_dist[dataset][a])\n",
    "            x.append(next_x)\n",
    "            y.append(val)\n",
    "        else:\n",
    "            x.append(a)\n",
    "            y.append(val)\n",
    "    plt.plot(x,y,label='Algorithm 2',linestyle='--',marker='o')\n",
    "    plt.plot([0.25], ps_size[dataset],marker='s',label='Paired Sweep')\n",
    "    plt.plot([0.25], fps_size[dataset],marker='+',label='Fair Paired Sweep')\n",
    "    plt.legend()\n",
    "    plt.title(dataset)\n",
    "    plt.xlabel(r'Color Participation Upper Bound ($a$)')\n",
    "    plt.ylabel('Density')\n",
    "    #plt.savefig(\"plots/individual/\"+dataset+\".eps\", format='eps', bbox_inches =\"tight\")\n",
    "    plt.show()\n",
    "    cnt += 1\n",
    "    if cnt > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot general statistics\n",
    "DenseSubgraph = [val for key,val in ds_size.items()]\n",
    "ds_a = []\n",
    "for dataset in ds_dist:\n",
    "    max_a = max(ds_dist[dataset])/sum(ds_dist[dataset])\n",
    "    ds_a.append( max_a )\n",
    "#print(np.mean(ds_a))\n",
    "plt.errorbar(np.mean(ds_a),np.mean(DenseSubgraph),xerr=np.std(ds_a),marker='x')\n",
    "print(\"1. Dense Subgraph\\t Mean:{:.2f}, Std:{:.2f}\".format(np.mean(DenseSubgraph),np.std(DenseSubgraph)))\n",
    "print(\"2. Algorithm 2\")\n",
    "x,y,err = [],[],[]\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    DS = [key_val[a] for key_val in list(algo2_size.values())]\n",
    "    print(\"alpha = {}\\tMean: {:.2f}, Std: {:.2f}\".format(a,np.mean(DS),np.std(DS)))\n",
    "    x.append(a)\n",
    "    y.append(np.mean(DS))\n",
    "    err.append(np.std(DS))\n",
    "plt.errorbar(x,y,yerr=err)\n",
    "plt.title('All Datasets')\n",
    "plt.xlabel('Color Participation Upper Bound (a)')\n",
    "plt.ylabel('Density')\n",
    "#plt.savefig(\"all_data.png\", bbox_inches =\"tight\")\n",
    "PS = [val for key,val in ps_size.items()]\n",
    "FPS = [val for key,val in fps_size.items()]\n",
    "print(\"3. Paired Sweep\\t Mean:{:.2f}, Std:{:.2f}\".format(np.mean(PS),np.std(PS)))\n",
    "print(\"4. (Fair) Paired Sweep\\t Mean:{:.2f}, Std:{:.2f}\".format(np.mean(FPS),np.std(FPS)))\n",
    "\n",
    "\n",
    "print(\" ---- Nodes -----\")\n",
    "ds_mean = np.mean(list(ds_nodes.values()))\n",
    "ds_std = np.std(list(ds_nodes.values()))\n",
    "print(\"1. Dense Subgraph\\t Mean:{:.2f}, Std:{:.2f}\".format(ds_mean,ds_std))\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    a_mean = np.mean([key_val[a] for key_val in list(algo2_nodes.values())])\n",
    "    a_std = np.std([key_val[a] for key_val in list(algo2_nodes.values())])\n",
    "    print(\"alpha = {}\\tMean: {:.2f}, Std: {:.2f}\".format(a,a_mean,a_std))\n",
    "ps_mean = np.mean(list(ps_nodes.values()))\n",
    "ps_std = np.std(list(ps_nodes.values()))\n",
    "\n",
    "fps_mean = np.mean(list(fps_nodes.values()))\n",
    "fps_std = np.std(list(fps_nodes.values()))\n",
    "\n",
    "print(\"3. Paired Sweep\\t Mean:{:.2f}, Std:{:.2f}\".format(ps_mean,ps_std))\n",
    "print(\"4. (Fair) Paired Sweep\\t Mean:{:.2f}, Std:{:.2f}\".format(fps_mean,fps_std))\n",
    "\n",
    "\n",
    "print(\" ---- Connected ----- \")\n",
    "ds_mean = np.sum(list(ds_connected.values()))\n",
    "print(\"1. Dense Subgraph\\t:{:.2f}\".format(ds_mean))\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    a_mean = np.sum([key_val[a] for key_val in list(algo2_connected.values())])\n",
    "    print(\"alpha = {}\\t: {:.2f}\".format(a,a_mean))\n",
    "ps_mean = np.sum(list(ps_connected.values()))\n",
    "\n",
    "fps_mean = np.sum(list(fps_connected.values()))\n",
    "\n",
    "print(\"3. Paired Sweep\\t Mean:{:.2f}\".format(ps_mean,ps_std))\n",
    "print(\"4. (Fair) Paired Sweep\\t Mean:{:.2f}\".format(fps_mean))\n",
    "\n",
    "print(\" ---- Diameter -----\")\n",
    "ds_g = set([v for v in ds_connected if ds_connected[v]])\n",
    "a_g = set([v for v in algo2_connected if algo2_connected[v][0.5]])\n",
    "f_g = set([v for v in fps_connected if fps_connected[v]])\n",
    "temp = ds_g.intersection(a_g)\n",
    "temp = temp.intersection(f_g)\n",
    "\n",
    "ds_mean = np.mean([ds_diameter[g] for g in temp])\n",
    "ds_std = np.std([ds_diameter[g] for g in temp])\n",
    "print(\"1. Dense Subgraph\\t Mean:{:.2f}, Std:{:.2f}\".format(ds_mean,ds_std))\n",
    "a_mean = np.mean([algo2_diameter[g][0.5] for g in temp])\n",
    "a_std = np.std([algo2_diameter[g][0.5] for g in temp])\n",
    "print(\"2. alpha = {}\\tMean: {:.2f}, Std: {:.2f}\".format(0.5,a_mean,a_std))\n",
    "fps_mean = np.mean([fps_diameter[g] for g in temp])\n",
    "fps_std = np.std([fps_diameter[g] for g in temp])\n",
    "print(\"3. (Fair) Paired Sweep\\t Mean:{:.2f}, Std:{:.2f}\".format(fps_mean,fps_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fairness distribution\n",
    "DS_results = defaultdict(list)\n",
    "for dataset in ds_dist:\n",
    "    f_dist = sorted(ds_dist[dataset])\n",
    "    sum_f_dist = sum(f_dist)\n",
    "    f_dist = [x/sum_f_dist for x in f_dist]\n",
    "    DS_results['0'].append(f_dist[0])\n",
    "    DS_results['1'].append(f_dist[1])\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(DS_results.values())\n",
    "ax.set_xticklabels(DS_results.keys())\n",
    "plt.title('Densest Subgraph')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(\"dense_subgraph_amazon_boxplot.eps\", format=\"eps\", bbox_inches =\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Do it with Algo2\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    temp_results = defaultdict(list)\n",
    "    for dataset in algo2_dist:\n",
    "        f_dist = sorted(algo2_dist[dataset][a])\n",
    "        sum_f_dist = sum(f_dist)\n",
    "        if sum_f_dist == 0:\n",
    "            continue\n",
    "        f_dist = [x/sum_f_dist for x in f_dist]\n",
    "        temp_results['0'].append(f_dist[0])\n",
    "        temp_results['1'].append(f_dist[1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(temp_results.values())\n",
    "    ax.set_xticklabels(temp_results.keys())\n",
    "    plt.title(r'$\\alpha = {}$'.format(a))\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel(\"Color (ordered by size)\")\n",
    "    plt.ylabel('Fraction of color in DS')\n",
    "    plt.savefig(str(a)+\"_boxplot_amazon.eps\", format=\"eps\", bbox_inches =\"tight\")\n",
    "    plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot averages for all colors\n",
    "# Plot DS\n",
    "x_ds = np.mean([max(v)/sum(v) for v in list(ds_dist.values())])\n",
    "y_ds = np.mean([v for v in list(ds_size.values())])\n",
    "plt.plot(x_ds,y_ds,marker='x',markersize=18,label=\"DSD\",color='black')\n",
    "\n",
    "\n",
    "# Plot FPS\n",
    "x_fps = 0.5\n",
    "y_fps = np.mean([v for v in list(fps_size.values())])\n",
    "plt.plot(x_fps,y_fps,marker='s',markersize=18,label=\"Fair PS\",color='red')\n",
    "\n",
    "# Plot PS\n",
    "x_ps = 0.5\n",
    "y_ps = np.mean([v for v in list(ps_size.values())])\n",
    "plt.plot(x_ps,y_ps,marker='^',markersize=18,label=\"PS\",color='green')\n",
    "\n",
    "# Plot Algo2\n",
    "x_a = []\n",
    "y_a = []\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    y_a.append( np.mean([v[a] for _,v in algo2_size.items()]) )\n",
    "    x_a.append( np.mean([max(v[a])/sum(v[a]) for _,v in algo2_dist.items()]) )\n",
    "plt.plot(x_a,y_a,marker='o',label='Algo. 2',color='blue')\n",
    "\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel(r'Color Participation Upper Bound ($\\alpha$)')\n",
    "plt.legend()\n",
    "plt.title('Amazon Products')\n",
    "plt.savefig('amazon_total.eps',format='eps', bbox_inches =\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot DS\n",
    "x_ds = np.mean([max(v)/sum(v) for v in list(ds_dist.values())])\n",
    "y_ds = np.mean([v for v in list(ds_size.values())])\n",
    "plt.plot(x_ds,y_ds/y_ds,marker='x',markersize=18,label=\"DSP\",color='black')\n",
    "\n",
    "\n",
    "# Plot FPS\n",
    "x_fps = 0.5\n",
    "y_fps = np.mean([v/y_ds for v in list(fps_size.values())])\n",
    "plt.plot(x_fps,y_fps,marker='s',markersize=18,label=\"Fair PS\",color='red')\n",
    "\n",
    "# Plot PS\n",
    "x_ps = 0.5\n",
    "y_ps = np.mean([v/y_ds for v in list(ps_size.values())])\n",
    "plt.plot(x_ps,y_ps,marker='^',markersize=18,label=\"PS\",color='green')\n",
    "\n",
    "# Plot Algorithm2\n",
    "x_a = []\n",
    "y_a = []\n",
    "for a in [0.5,0.55,0.6,0.7,0.8,0.9,0.99]:\n",
    "    y_a.append( np.mean([v[a]/y_ds for _,v in algo2_size.items()]) )\n",
    "    x_a.append( np.mean([max(v[a])/sum(v[a]) for _,v in algo2_dist.items()]) )\n",
    "plt.plot(x_a,y_a,marker='o',label='Algo. 2',color='blue')\n",
    "\n",
    "plt.ylabel('Normalized Density')\n",
    "plt.xlabel(r'Color Participation Upper Bound ($\\alpha$)')\n",
    "plt.ylim(0,1.05)\n",
    "plt.legend(loc='lower left')\n",
    "plt.title('Amazon Products')\n",
    "plt.savefig('amazon_total_normaled.eps',format='eps', bbox_inches =\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
